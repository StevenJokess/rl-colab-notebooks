{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "pretraining.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenJokess/rl-colab-notebooks/blob/sb3/pretraining(colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShofuYy-8dLs"
      },
      "source": [
        "## Stable Baselines3 - Pretraining with Behavior Cloning\n",
        "\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to record expert data, then pre-train an agent using this data\n",
        "and finally continue training with Stable-Baselines3.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "Notebook originally created by [skervim](https://github.com/skervim)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwJ1LHlN9E33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ea0617-0b1d-45a1-e6ce-c3707050fe9e"
      },
      "source": [
        "# For Box2D env\n",
        "!apt-get install swig\n",
        "!pip install gym[box2d]\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[box2d]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.6/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.7.0+cu101)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (1.1.5)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (7.0.0)\n",
            "Requirement already satisfied: tensorboard; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (2.4.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines3[extra]) (2.8.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.17.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.12.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]) (51.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgx4AMZo8anP"
      },
      "source": [
        "import gym\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_1ZNBum8ane"
      },
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaz2Szrl8anx"
      },
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9lr70R9eU9"
      },
      "source": [
        "# Example for continuous actions\n",
        "# env_id = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "# Example for discrete actions\n",
        "env_id = \"CartPole-v1\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh88d4oR8an6"
      },
      "source": [
        "env = gym.make(env_id)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKCgHCc_8aoB"
      },
      "source": [
        "## Train Expert Model\n",
        "\n",
        "We create an expert RL agent and let it learn to solve a task by interacting with the evironment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkmIST0r8aoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebecae6f-de9b-4349-9821-4b6791b10372"
      },
      "source": [
        "ppo_expert = PPO('MlpPolicy', env_id, verbose=1, create_eval_env=True)\n",
        "ppo_expert.learn(total_timesteps=30, eval_freq=10)# total_timesteps=3e4，eval_freq=1000\n",
        "ppo_expert.save(\"ppo_expert\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Eval num_timesteps=10, episode_reward=48.80 +/- 17.67\n",
            "Episode length: 48.80 +/- 17.67\n",
            "New best mean reward!\n",
            "Eval num_timesteps=20, episode_reward=55.80 +/- 11.63\n",
            "Episode length: 55.80 +/- 11.63\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30, episode_reward=55.40 +/- 19.19\n",
            "Episode length: 55.40 +/- 19.19\n",
            "Eval num_timesteps=40, episode_reward=46.80 +/- 11.14\n",
            "Episode length: 46.80 +/- 11.14\n",
            "Eval num_timesteps=50, episode_reward=51.00 +/- 17.01\n",
            "Episode length: 51.00 +/- 17.01\n",
            "Eval num_timesteps=60, episode_reward=55.20 +/- 15.35\n",
            "Episode length: 55.20 +/- 15.35\n",
            "Eval num_timesteps=70, episode_reward=55.20 +/- 14.59\n",
            "Episode length: 55.20 +/- 14.59\n",
            "Eval num_timesteps=80, episode_reward=59.60 +/- 9.71\n",
            "Episode length: 59.60 +/- 9.71\n",
            "New best mean reward!\n",
            "Eval num_timesteps=90, episode_reward=67.80 +/- 21.14\n",
            "Episode length: 67.80 +/- 21.14\n",
            "New best mean reward!\n",
            "Eval num_timesteps=100, episode_reward=53.40 +/- 9.81\n",
            "Episode length: 53.40 +/- 9.81\n",
            "Eval num_timesteps=110, episode_reward=49.80 +/- 10.87\n",
            "Episode length: 49.80 +/- 10.87\n",
            "Eval num_timesteps=120, episode_reward=63.00 +/- 17.46\n",
            "Episode length: 63.00 +/- 17.46\n",
            "Eval num_timesteps=130, episode_reward=48.60 +/- 7.86\n",
            "Episode length: 48.60 +/- 7.86\n",
            "Eval num_timesteps=140, episode_reward=57.00 +/- 18.60\n",
            "Episode length: 57.00 +/- 18.60\n",
            "Eval num_timesteps=150, episode_reward=64.20 +/- 12.38\n",
            "Episode length: 64.20 +/- 12.38\n",
            "Eval num_timesteps=160, episode_reward=55.60 +/- 11.16\n",
            "Episode length: 55.60 +/- 11.16\n",
            "Eval num_timesteps=170, episode_reward=66.20 +/- 20.19\n",
            "Episode length: 66.20 +/- 20.19\n",
            "Eval num_timesteps=180, episode_reward=43.20 +/- 4.02\n",
            "Episode length: 43.20 +/- 4.02\n",
            "Eval num_timesteps=190, episode_reward=59.60 +/- 7.31\n",
            "Episode length: 59.60 +/- 7.31\n",
            "Eval num_timesteps=200, episode_reward=60.00 +/- 18.43\n",
            "Episode length: 60.00 +/- 18.43\n",
            "Eval num_timesteps=210, episode_reward=49.80 +/- 8.42\n",
            "Episode length: 49.80 +/- 8.42\n",
            "Eval num_timesteps=220, episode_reward=58.00 +/- 29.20\n",
            "Episode length: 58.00 +/- 29.20\n",
            "Eval num_timesteps=230, episode_reward=58.00 +/- 14.59\n",
            "Episode length: 58.00 +/- 14.59\n",
            "Eval num_timesteps=240, episode_reward=68.60 +/- 20.98\n",
            "Episode length: 68.60 +/- 20.98\n",
            "New best mean reward!\n",
            "Eval num_timesteps=250, episode_reward=59.00 +/- 11.95\n",
            "Episode length: 59.00 +/- 11.95\n",
            "Eval num_timesteps=260, episode_reward=71.40 +/- 14.89\n",
            "Episode length: 71.40 +/- 14.89\n",
            "New best mean reward!\n",
            "Eval num_timesteps=270, episode_reward=47.00 +/- 6.54\n",
            "Episode length: 47.00 +/- 6.54\n",
            "Eval num_timesteps=280, episode_reward=55.00 +/- 5.90\n",
            "Episode length: 55.00 +/- 5.90\n",
            "Eval num_timesteps=290, episode_reward=52.40 +/- 9.05\n",
            "Episode length: 52.40 +/- 9.05\n",
            "Eval num_timesteps=300, episode_reward=60.60 +/- 16.30\n",
            "Episode length: 60.60 +/- 16.30\n",
            "Eval num_timesteps=310, episode_reward=53.20 +/- 11.91\n",
            "Episode length: 53.20 +/- 11.91\n",
            "Eval num_timesteps=320, episode_reward=53.80 +/- 12.54\n",
            "Episode length: 53.80 +/- 12.54\n",
            "Eval num_timesteps=330, episode_reward=57.80 +/- 9.00\n",
            "Episode length: 57.80 +/- 9.00\n",
            "Eval num_timesteps=340, episode_reward=46.20 +/- 9.26\n",
            "Episode length: 46.20 +/- 9.26\n",
            "Eval num_timesteps=350, episode_reward=58.00 +/- 18.20\n",
            "Episode length: 58.00 +/- 18.20\n",
            "Eval num_timesteps=360, episode_reward=56.00 +/- 13.42\n",
            "Episode length: 56.00 +/- 13.42\n",
            "Eval num_timesteps=370, episode_reward=60.60 +/- 11.88\n",
            "Episode length: 60.60 +/- 11.88\n",
            "Eval num_timesteps=380, episode_reward=64.80 +/- 13.06\n",
            "Episode length: 64.80 +/- 13.06\n",
            "Eval num_timesteps=390, episode_reward=49.80 +/- 9.39\n",
            "Episode length: 49.80 +/- 9.39\n",
            "Eval num_timesteps=400, episode_reward=52.80 +/- 9.04\n",
            "Episode length: 52.80 +/- 9.04\n",
            "Eval num_timesteps=410, episode_reward=53.20 +/- 12.27\n",
            "Episode length: 53.20 +/- 12.27\n",
            "Eval num_timesteps=420, episode_reward=47.40 +/- 9.07\n",
            "Episode length: 47.40 +/- 9.07\n",
            "Eval num_timesteps=430, episode_reward=55.60 +/- 20.48\n",
            "Episode length: 55.60 +/- 20.48\n",
            "Eval num_timesteps=440, episode_reward=56.80 +/- 10.85\n",
            "Episode length: 56.80 +/- 10.85\n",
            "Eval num_timesteps=450, episode_reward=55.00 +/- 18.03\n",
            "Episode length: 55.00 +/- 18.03\n",
            "Eval num_timesteps=460, episode_reward=52.60 +/- 9.00\n",
            "Episode length: 52.60 +/- 9.00\n",
            "Eval num_timesteps=470, episode_reward=65.60 +/- 15.89\n",
            "Episode length: 65.60 +/- 15.89\n",
            "Eval num_timesteps=480, episode_reward=54.40 +/- 8.57\n",
            "Episode length: 54.40 +/- 8.57\n",
            "Eval num_timesteps=490, episode_reward=43.20 +/- 5.98\n",
            "Episode length: 43.20 +/- 5.98\n",
            "Eval num_timesteps=500, episode_reward=53.80 +/- 10.01\n",
            "Episode length: 53.80 +/- 10.01\n",
            "Eval num_timesteps=510, episode_reward=57.60 +/- 10.76\n",
            "Episode length: 57.60 +/- 10.76\n",
            "Eval num_timesteps=520, episode_reward=64.00 +/- 10.68\n",
            "Episode length: 64.00 +/- 10.68\n",
            "Eval num_timesteps=530, episode_reward=46.20 +/- 4.83\n",
            "Episode length: 46.20 +/- 4.83\n",
            "Eval num_timesteps=540, episode_reward=55.40 +/- 20.92\n",
            "Episode length: 55.40 +/- 20.92\n",
            "Eval num_timesteps=550, episode_reward=55.40 +/- 11.11\n",
            "Episode length: 55.40 +/- 11.11\n",
            "Eval num_timesteps=560, episode_reward=71.40 +/- 23.06\n",
            "Episode length: 71.40 +/- 23.06\n",
            "Eval num_timesteps=570, episode_reward=61.40 +/- 21.90\n",
            "Episode length: 61.40 +/- 21.90\n",
            "Eval num_timesteps=580, episode_reward=49.60 +/- 10.11\n",
            "Episode length: 49.60 +/- 10.11\n",
            "Eval num_timesteps=590, episode_reward=59.00 +/- 10.02\n",
            "Episode length: 59.00 +/- 10.02\n",
            "Eval num_timesteps=600, episode_reward=47.20 +/- 5.60\n",
            "Episode length: 47.20 +/- 5.60\n",
            "Eval num_timesteps=610, episode_reward=56.80 +/- 14.09\n",
            "Episode length: 56.80 +/- 14.09\n",
            "Eval num_timesteps=620, episode_reward=56.40 +/- 16.24\n",
            "Episode length: 56.40 +/- 16.24\n",
            "Eval num_timesteps=630, episode_reward=49.80 +/- 10.46\n",
            "Episode length: 49.80 +/- 10.46\n",
            "Eval num_timesteps=640, episode_reward=58.40 +/- 7.42\n",
            "Episode length: 58.40 +/- 7.42\n",
            "Eval num_timesteps=650, episode_reward=49.20 +/- 7.39\n",
            "Episode length: 49.20 +/- 7.39\n",
            "Eval num_timesteps=660, episode_reward=55.40 +/- 15.72\n",
            "Episode length: 55.40 +/- 15.72\n",
            "Eval num_timesteps=670, episode_reward=44.60 +/- 7.50\n",
            "Episode length: 44.60 +/- 7.50\n",
            "Eval num_timesteps=680, episode_reward=44.60 +/- 5.12\n",
            "Episode length: 44.60 +/- 5.12\n",
            "Eval num_timesteps=690, episode_reward=50.00 +/- 14.41\n",
            "Episode length: 50.00 +/- 14.41\n",
            "Eval num_timesteps=700, episode_reward=64.80 +/- 24.08\n",
            "Episode length: 64.80 +/- 24.08\n",
            "Eval num_timesteps=710, episode_reward=54.40 +/- 10.87\n",
            "Episode length: 54.40 +/- 10.87\n",
            "Eval num_timesteps=720, episode_reward=58.60 +/- 21.40\n",
            "Episode length: 58.60 +/- 21.40\n",
            "Eval num_timesteps=730, episode_reward=59.60 +/- 12.75\n",
            "Episode length: 59.60 +/- 12.75\n",
            "Eval num_timesteps=740, episode_reward=60.40 +/- 14.50\n",
            "Episode length: 60.40 +/- 14.50\n",
            "Eval num_timesteps=750, episode_reward=55.20 +/- 9.11\n",
            "Episode length: 55.20 +/- 9.11\n",
            "Eval num_timesteps=760, episode_reward=54.40 +/- 11.15\n",
            "Episode length: 54.40 +/- 11.15\n",
            "Eval num_timesteps=770, episode_reward=72.40 +/- 18.07\n",
            "Episode length: 72.40 +/- 18.07\n",
            "New best mean reward!\n",
            "Eval num_timesteps=780, episode_reward=52.80 +/- 12.64\n",
            "Episode length: 52.80 +/- 12.64\n",
            "Eval num_timesteps=790, episode_reward=59.20 +/- 9.60\n",
            "Episode length: 59.20 +/- 9.60\n",
            "Eval num_timesteps=800, episode_reward=62.20 +/- 24.03\n",
            "Episode length: 62.20 +/- 24.03\n",
            "Eval num_timesteps=810, episode_reward=63.20 +/- 19.50\n",
            "Episode length: 63.20 +/- 19.50\n",
            "Eval num_timesteps=820, episode_reward=51.40 +/- 13.35\n",
            "Episode length: 51.40 +/- 13.35\n",
            "Eval num_timesteps=830, episode_reward=63.40 +/- 13.81\n",
            "Episode length: 63.40 +/- 13.81\n",
            "Eval num_timesteps=840, episode_reward=64.20 +/- 13.96\n",
            "Episode length: 64.20 +/- 13.96\n",
            "Eval num_timesteps=850, episode_reward=56.40 +/- 13.11\n",
            "Episode length: 56.40 +/- 13.11\n",
            "Eval num_timesteps=860, episode_reward=86.20 +/- 27.37\n",
            "Episode length: 86.20 +/- 27.37\n",
            "New best mean reward!\n",
            "Eval num_timesteps=870, episode_reward=64.60 +/- 24.07\n",
            "Episode length: 64.60 +/- 24.07\n",
            "Eval num_timesteps=880, episode_reward=55.00 +/- 14.53\n",
            "Episode length: 55.00 +/- 14.53\n",
            "Eval num_timesteps=890, episode_reward=63.80 +/- 16.27\n",
            "Episode length: 63.80 +/- 16.27\n",
            "Eval num_timesteps=900, episode_reward=62.80 +/- 20.73\n",
            "Episode length: 62.80 +/- 20.73\n",
            "Eval num_timesteps=910, episode_reward=58.40 +/- 14.50\n",
            "Episode length: 58.40 +/- 14.50\n",
            "Eval num_timesteps=920, episode_reward=50.60 +/- 8.40\n",
            "Episode length: 50.60 +/- 8.40\n",
            "Eval num_timesteps=930, episode_reward=60.80 +/- 12.40\n",
            "Episode length: 60.80 +/- 12.40\n",
            "Eval num_timesteps=940, episode_reward=62.80 +/- 20.50\n",
            "Episode length: 62.80 +/- 20.50\n",
            "Eval num_timesteps=950, episode_reward=46.40 +/- 8.33\n",
            "Episode length: 46.40 +/- 8.33\n",
            "Eval num_timesteps=960, episode_reward=59.00 +/- 9.63\n",
            "Episode length: 59.00 +/- 9.63\n",
            "Eval num_timesteps=970, episode_reward=46.40 +/- 9.00\n",
            "Episode length: 46.40 +/- 9.00\n",
            "Eval num_timesteps=980, episode_reward=64.00 +/- 31.81\n",
            "Episode length: 64.00 +/- 31.81\n",
            "Eval num_timesteps=990, episode_reward=54.80 +/- 15.73\n",
            "Episode length: 54.80 +/- 15.73\n",
            "Eval num_timesteps=1000, episode_reward=56.80 +/- 13.03\n",
            "Episode length: 56.80 +/- 13.03\n",
            "Eval num_timesteps=1010, episode_reward=57.80 +/- 9.70\n",
            "Episode length: 57.80 +/- 9.70\n",
            "Eval num_timesteps=1020, episode_reward=48.20 +/- 11.92\n",
            "Episode length: 48.20 +/- 11.92\n",
            "Eval num_timesteps=1030, episode_reward=51.40 +/- 10.71\n",
            "Episode length: 51.40 +/- 10.71\n",
            "Eval num_timesteps=1040, episode_reward=52.60 +/- 13.37\n",
            "Episode length: 52.60 +/- 13.37\n",
            "Eval num_timesteps=1050, episode_reward=59.60 +/- 8.11\n",
            "Episode length: 59.60 +/- 8.11\n",
            "Eval num_timesteps=1060, episode_reward=49.20 +/- 13.50\n",
            "Episode length: 49.20 +/- 13.50\n",
            "Eval num_timesteps=1070, episode_reward=57.40 +/- 17.50\n",
            "Episode length: 57.40 +/- 17.50\n",
            "Eval num_timesteps=1080, episode_reward=59.00 +/- 15.58\n",
            "Episode length: 59.00 +/- 15.58\n",
            "Eval num_timesteps=1090, episode_reward=49.80 +/- 5.42\n",
            "Episode length: 49.80 +/- 5.42\n",
            "Eval num_timesteps=1100, episode_reward=48.80 +/- 4.71\n",
            "Episode length: 48.80 +/- 4.71\n",
            "Eval num_timesteps=1110, episode_reward=48.60 +/- 5.16\n",
            "Episode length: 48.60 +/- 5.16\n",
            "Eval num_timesteps=1120, episode_reward=52.00 +/- 6.60\n",
            "Episode length: 52.00 +/- 6.60\n",
            "Eval num_timesteps=1130, episode_reward=63.40 +/- 20.92\n",
            "Episode length: 63.40 +/- 20.92\n",
            "Eval num_timesteps=1140, episode_reward=71.40 +/- 32.13\n",
            "Episode length: 71.40 +/- 32.13\n",
            "Eval num_timesteps=1150, episode_reward=60.60 +/- 14.92\n",
            "Episode length: 60.60 +/- 14.92\n",
            "Eval num_timesteps=1160, episode_reward=59.20 +/- 16.77\n",
            "Episode length: 59.20 +/- 16.77\n",
            "Eval num_timesteps=1170, episode_reward=58.00 +/- 14.72\n",
            "Episode length: 58.00 +/- 14.72\n",
            "Eval num_timesteps=1180, episode_reward=46.00 +/- 12.74\n",
            "Episode length: 46.00 +/- 12.74\n",
            "Eval num_timesteps=1190, episode_reward=58.20 +/- 15.28\n",
            "Episode length: 58.20 +/- 15.28\n",
            "Eval num_timesteps=1200, episode_reward=53.40 +/- 13.57\n",
            "Episode length: 53.40 +/- 13.57\n",
            "Eval num_timesteps=1210, episode_reward=58.60 +/- 14.04\n",
            "Episode length: 58.60 +/- 14.04\n",
            "Eval num_timesteps=1220, episode_reward=49.80 +/- 11.70\n",
            "Episode length: 49.80 +/- 11.70\n",
            "Eval num_timesteps=1230, episode_reward=56.40 +/- 13.02\n",
            "Episode length: 56.40 +/- 13.02\n",
            "Eval num_timesteps=1240, episode_reward=54.60 +/- 8.55\n",
            "Episode length: 54.60 +/- 8.55\n",
            "Eval num_timesteps=1250, episode_reward=62.00 +/- 6.42\n",
            "Episode length: 62.00 +/- 6.42\n",
            "Eval num_timesteps=1260, episode_reward=58.60 +/- 10.07\n",
            "Episode length: 58.60 +/- 10.07\n",
            "Eval num_timesteps=1270, episode_reward=62.00 +/- 16.16\n",
            "Episode length: 62.00 +/- 16.16\n",
            "Eval num_timesteps=1280, episode_reward=53.80 +/- 9.00\n",
            "Episode length: 53.80 +/- 9.00\n",
            "Eval num_timesteps=1290, episode_reward=64.20 +/- 17.99\n",
            "Episode length: 64.20 +/- 17.99\n",
            "Eval num_timesteps=1300, episode_reward=58.20 +/- 12.75\n",
            "Episode length: 58.20 +/- 12.75\n",
            "Eval num_timesteps=1310, episode_reward=55.00 +/- 13.67\n",
            "Episode length: 55.00 +/- 13.67\n",
            "Eval num_timesteps=1320, episode_reward=56.60 +/- 18.38\n",
            "Episode length: 56.60 +/- 18.38\n",
            "Eval num_timesteps=1330, episode_reward=53.80 +/- 16.33\n",
            "Episode length: 53.80 +/- 16.33\n",
            "Eval num_timesteps=1340, episode_reward=60.20 +/- 10.46\n",
            "Episode length: 60.20 +/- 10.46\n",
            "Eval num_timesteps=1350, episode_reward=52.60 +/- 11.20\n",
            "Episode length: 52.60 +/- 11.20\n",
            "Eval num_timesteps=1360, episode_reward=56.20 +/- 14.25\n",
            "Episode length: 56.20 +/- 14.25\n",
            "Eval num_timesteps=1370, episode_reward=52.60 +/- 11.71\n",
            "Episode length: 52.60 +/- 11.71\n",
            "Eval num_timesteps=1380, episode_reward=48.80 +/- 5.84\n",
            "Episode length: 48.80 +/- 5.84\n",
            "Eval num_timesteps=1390, episode_reward=59.20 +/- 25.05\n",
            "Episode length: 59.20 +/- 25.05\n",
            "Eval num_timesteps=1400, episode_reward=51.00 +/- 12.79\n",
            "Episode length: 51.00 +/- 12.79\n",
            "Eval num_timesteps=1410, episode_reward=59.80 +/- 17.06\n",
            "Episode length: 59.80 +/- 17.06\n",
            "Eval num_timesteps=1420, episode_reward=55.20 +/- 16.00\n",
            "Episode length: 55.20 +/- 16.00\n",
            "Eval num_timesteps=1430, episode_reward=58.00 +/- 13.18\n",
            "Episode length: 58.00 +/- 13.18\n",
            "Eval num_timesteps=1440, episode_reward=48.40 +/- 7.89\n",
            "Episode length: 48.40 +/- 7.89\n",
            "Eval num_timesteps=1450, episode_reward=51.80 +/- 10.19\n",
            "Episode length: 51.80 +/- 10.19\n",
            "Eval num_timesteps=1460, episode_reward=44.20 +/- 5.27\n",
            "Episode length: 44.20 +/- 5.27\n",
            "Eval num_timesteps=1470, episode_reward=49.20 +/- 8.13\n",
            "Episode length: 49.20 +/- 8.13\n",
            "Eval num_timesteps=1480, episode_reward=60.40 +/- 8.85\n",
            "Episode length: 60.40 +/- 8.85\n",
            "Eval num_timesteps=1490, episode_reward=67.60 +/- 31.42\n",
            "Episode length: 67.60 +/- 31.42\n",
            "Eval num_timesteps=1500, episode_reward=53.40 +/- 19.20\n",
            "Episode length: 53.40 +/- 19.20\n",
            "Eval num_timesteps=1510, episode_reward=63.40 +/- 19.83\n",
            "Episode length: 63.40 +/- 19.83\n",
            "Eval num_timesteps=1520, episode_reward=53.20 +/- 6.94\n",
            "Episode length: 53.20 +/- 6.94\n",
            "Eval num_timesteps=1530, episode_reward=49.20 +/- 10.40\n",
            "Episode length: 49.20 +/- 10.40\n",
            "Eval num_timesteps=1540, episode_reward=54.40 +/- 9.95\n",
            "Episode length: 54.40 +/- 9.95\n",
            "Eval num_timesteps=1550, episode_reward=44.60 +/- 5.46\n",
            "Episode length: 44.60 +/- 5.46\n",
            "Eval num_timesteps=1560, episode_reward=60.40 +/- 18.26\n",
            "Episode length: 60.40 +/- 18.26\n",
            "Eval num_timesteps=1570, episode_reward=62.40 +/- 16.41\n",
            "Episode length: 62.40 +/- 16.41\n",
            "Eval num_timesteps=1580, episode_reward=66.20 +/- 10.57\n",
            "Episode length: 66.20 +/- 10.57\n",
            "Eval num_timesteps=1590, episode_reward=62.60 +/- 16.74\n",
            "Episode length: 62.60 +/- 16.74\n",
            "Eval num_timesteps=1600, episode_reward=53.60 +/- 10.86\n",
            "Episode length: 53.60 +/- 10.86\n",
            "Eval num_timesteps=1610, episode_reward=55.80 +/- 12.42\n",
            "Episode length: 55.80 +/- 12.42\n",
            "Eval num_timesteps=1620, episode_reward=44.80 +/- 8.66\n",
            "Episode length: 44.80 +/- 8.66\n",
            "Eval num_timesteps=1630, episode_reward=48.80 +/- 15.28\n",
            "Episode length: 48.80 +/- 15.28\n",
            "Eval num_timesteps=1640, episode_reward=58.00 +/- 23.09\n",
            "Episode length: 58.00 +/- 23.09\n",
            "Eval num_timesteps=1650, episode_reward=52.40 +/- 17.78\n",
            "Episode length: 52.40 +/- 17.78\n",
            "Eval num_timesteps=1660, episode_reward=68.00 +/- 26.93\n",
            "Episode length: 68.00 +/- 26.93\n",
            "Eval num_timesteps=1670, episode_reward=62.60 +/- 16.61\n",
            "Episode length: 62.60 +/- 16.61\n",
            "Eval num_timesteps=1680, episode_reward=62.80 +/- 9.22\n",
            "Episode length: 62.80 +/- 9.22\n",
            "Eval num_timesteps=1690, episode_reward=62.80 +/- 17.47\n",
            "Episode length: 62.80 +/- 17.47\n",
            "Eval num_timesteps=1700, episode_reward=44.40 +/- 5.92\n",
            "Episode length: 44.40 +/- 5.92\n",
            "Eval num_timesteps=1710, episode_reward=56.00 +/- 12.13\n",
            "Episode length: 56.00 +/- 12.13\n",
            "Eval num_timesteps=1720, episode_reward=84.60 +/- 31.00\n",
            "Episode length: 84.60 +/- 31.00\n",
            "Eval num_timesteps=1730, episode_reward=59.80 +/- 12.22\n",
            "Episode length: 59.80 +/- 12.22\n",
            "Eval num_timesteps=1740, episode_reward=56.60 +/- 5.57\n",
            "Episode length: 56.60 +/- 5.57\n",
            "Eval num_timesteps=1750, episode_reward=55.00 +/- 16.16\n",
            "Episode length: 55.00 +/- 16.16\n",
            "Eval num_timesteps=1760, episode_reward=60.40 +/- 18.61\n",
            "Episode length: 60.40 +/- 18.61\n",
            "Eval num_timesteps=1770, episode_reward=69.80 +/- 20.55\n",
            "Episode length: 69.80 +/- 20.55\n",
            "Eval num_timesteps=1780, episode_reward=64.60 +/- 19.56\n",
            "Episode length: 64.60 +/- 19.56\n",
            "Eval num_timesteps=1790, episode_reward=70.00 +/- 16.90\n",
            "Episode length: 70.00 +/- 16.90\n",
            "Eval num_timesteps=1800, episode_reward=51.60 +/- 9.75\n",
            "Episode length: 51.60 +/- 9.75\n",
            "Eval num_timesteps=1810, episode_reward=49.40 +/- 7.17\n",
            "Episode length: 49.40 +/- 7.17\n",
            "Eval num_timesteps=1820, episode_reward=47.80 +/- 13.89\n",
            "Episode length: 47.80 +/- 13.89\n",
            "Eval num_timesteps=1830, episode_reward=48.40 +/- 8.98\n",
            "Episode length: 48.40 +/- 8.98\n",
            "Eval num_timesteps=1840, episode_reward=49.20 +/- 9.33\n",
            "Episode length: 49.20 +/- 9.33\n",
            "Eval num_timesteps=1850, episode_reward=72.00 +/- 19.11\n",
            "Episode length: 72.00 +/- 19.11\n",
            "Eval num_timesteps=1860, episode_reward=47.60 +/- 7.09\n",
            "Episode length: 47.60 +/- 7.09\n",
            "Eval num_timesteps=1870, episode_reward=56.80 +/- 23.20\n",
            "Episode length: 56.80 +/- 23.20\n",
            "Eval num_timesteps=1880, episode_reward=49.00 +/- 9.21\n",
            "Episode length: 49.00 +/- 9.21\n",
            "Eval num_timesteps=1890, episode_reward=61.60 +/- 20.57\n",
            "Episode length: 61.60 +/- 20.57\n",
            "Eval num_timesteps=1900, episode_reward=53.20 +/- 12.70\n",
            "Episode length: 53.20 +/- 12.70\n",
            "Eval num_timesteps=1910, episode_reward=69.20 +/- 12.95\n",
            "Episode length: 69.20 +/- 12.95\n",
            "Eval num_timesteps=1920, episode_reward=48.40 +/- 11.39\n",
            "Episode length: 48.40 +/- 11.39\n",
            "Eval num_timesteps=1930, episode_reward=57.00 +/- 12.54\n",
            "Episode length: 57.00 +/- 12.54\n",
            "Eval num_timesteps=1940, episode_reward=69.20 +/- 34.73\n",
            "Episode length: 69.20 +/- 34.73\n",
            "Eval num_timesteps=1950, episode_reward=63.00 +/- 19.20\n",
            "Episode length: 63.00 +/- 19.20\n",
            "Eval num_timesteps=1960, episode_reward=61.60 +/- 4.88\n",
            "Episode length: 61.60 +/- 4.88\n",
            "Eval num_timesteps=1970, episode_reward=59.80 +/- 13.86\n",
            "Episode length: 59.80 +/- 13.86\n",
            "Eval num_timesteps=1980, episode_reward=85.20 +/- 28.88\n",
            "Episode length: 85.20 +/- 28.88\n",
            "Eval num_timesteps=1990, episode_reward=54.80 +/- 4.66\n",
            "Episode length: 54.80 +/- 4.66\n",
            "Eval num_timesteps=2000, episode_reward=51.60 +/- 13.69\n",
            "Episode length: 51.60 +/- 13.69\n",
            "Eval num_timesteps=2010, episode_reward=54.60 +/- 14.89\n",
            "Episode length: 54.60 +/- 14.89\n",
            "Eval num_timesteps=2020, episode_reward=65.20 +/- 12.35\n",
            "Episode length: 65.20 +/- 12.35\n",
            "Eval num_timesteps=2030, episode_reward=51.00 +/- 12.38\n",
            "Episode length: 51.00 +/- 12.38\n",
            "Eval num_timesteps=2040, episode_reward=61.20 +/- 8.70\n",
            "Episode length: 61.20 +/- 8.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61.2     |\n",
            "|    mean_reward     | 61.2     |\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 25.1     |\n",
            "|    ep_rew_mean     | 25.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 44       |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlyTfGAQ_Az1"
      },
      "source": [
        "check the performance of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_rVEjC0_AQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c8d193-70fc-463e-c6e8-02b194364ca5"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward = 250.1 +/- 171.31459365739977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Kv6v_V8aoJ"
      },
      "source": [
        "## Create Student\n",
        "\n",
        "We also create a student RL agent, which will later be trained with the expert dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLdLPUeC8aoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3977c469-7ec3-4556-b83a-30268f9259f9"
      },
      "source": [
        "a2c_student = A2C('MlpPolicy', env_id, verbose=1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdW8_41-OcXn"
      },
      "source": [
        "# only valid for continuous actions\n",
        "# sac_student = SAC('MlpPolicy', env_id, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3GuNxcU8aoT"
      },
      "source": [
        "\n",
        "We now let our expert interact with the environment (except we already have expert data) and store resultant expert observations and actions to build an expert dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emodyZDW8aoU"
      },
      "source": [
        "num_interactions = int(4e4)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3I_2s808aoZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b007787-c2fb-427a-f600-684c5c833d26"
      },
      "source": [
        "if isinstance(env.action_space, gym.spaces.Box):\n",
        "  expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "  expert_actions = np.empty((num_interactions,) + (env.action_space.shape[0],))\n",
        "\n",
        "else:\n",
        "  expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "  expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "for i in tqdm(range(num_interactions)):\n",
        "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
        "    expert_observations[i] = obs\n",
        "    expert_actions[i] = action\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        obs = env.reset()\n",
        "\n",
        "np.savez_compressed(\n",
        "    \"expert_data\",\n",
        "    expert_actions=expert_actions,\n",
        "    expert_observations=expert_observations,\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40000/40000 [00:29<00:00, 1359.12it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toKEQE9i8aof"
      },
      "source": [
        "\n",
        "\n",
        "- To seamlessly use PyTorch in the training process, we subclass an `ExpertDataset` from PyTorch's base `Dataset`.\n",
        "- Note that we initialize the dataset with the previously generated expert observations and actions.\n",
        "- We further implement Python's `__getitem__` and `__len__` magic functions to allow PyTorch's dataset-handling to access arbitrary rows in the dataset and inform it about the length of the dataset.\n",
        "- For more information about PyTorch's datasets, you can read: https://pytorch.org/docs/stable/data.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT72bR1i8aog"
      },
      "source": [
        "from torch.utils.data.dataset import Dataset, random_split"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUetr5vl8aom"
      },
      "source": [
        "class ExpertDataSet(Dataset):\n",
        "    def __init__(self, expert_observations, expert_actions):\n",
        "        self.observations = expert_observations\n",
        "        self.actions = expert_actions\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return (self.observations[index], self.actions[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bNAhXp8aor"
      },
      "source": [
        "\n",
        "\n",
        "We now instantiate the `ExpertDataSet` and split it into training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIdT-zMV8aot"
      },
      "source": [
        "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
        "\n",
        "train_size = int(0.8 * len(expert_dataset))\n",
        "\n",
        "test_size = len(expert_dataset) - train_size\n",
        "\n",
        "train_expert_dataset, test_expert_dataset = random_split(\n",
        "    expert_dataset, [train_size, test_size]\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LgmtFFq8aox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc18e9cc-5155-4ff5-d90a-da66373cef2d"
      },
      "source": [
        "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
        "print(\"train_expert_dataset: \", len(train_expert_dataset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_expert_dataset:  8000\n",
            "train_expert_dataset:  32000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v8PhG2r8ao4"
      },
      "source": [
        "\n",
        "\n",
        "NOTE: The supervised learning section of this code is adapted from: https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "1. We extract the policy network of our RL student agent.\n",
        "2. We load the (labeled) expert dataset containing expert observations as inputs and expert actions as targets.\n",
        "3. We perform supervised learning, that is, we adjust the policy network's parameters such that given expert observations as inputs to the network, its outputs match the targets (expert actions).\n",
        "By training the policy network in this way the corresponding RL student agent is taught to behave like the expert agent that was used to created the expert dataset (Behavior Cloning).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwUhCTGU8ao5"
      },
      "source": [
        "def pretrain_agent(\n",
        "    student,\n",
        "    batch_size=64,\n",
        "    epochs=10,# 1000\n",
        "    scheduler_gamma=0.7,\n",
        "    learning_rate=1.0,\n",
        "    log_interval=100,\n",
        "    no_cuda=True,\n",
        "    seed=1,\n",
        "    test_batch_size=64,\n",
        "):\n",
        "    use_cuda = not no_cuda and th.cuda.is_available()\n",
        "    th.manual_seed(seed)\n",
        "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "    if isinstance(env.action_space, gym.spaces.Box):\n",
        "      criterion = nn.MSELoss()\n",
        "    else:\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Extract initial policy\n",
        "    model = student.policy.to(device)\n",
        "\n",
        "    def train(model, device, train_loader, optimizer):\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if isinstance(env.action_space, gym.spaces.Box):\n",
        "              # A2C/PPO policy outputs actions, values, log_prob\n",
        "              # SAC/TD3 policy outputs actions only\n",
        "              if isinstance(student, (A2C, PPO)):\n",
        "                action, _, _ = model(data)\n",
        "              else:\n",
        "                # SAC/TD3:\n",
        "                action = model(data)\n",
        "              action_prediction = action.double()\n",
        "            else:\n",
        "              # Retrieve the logits for A2C/PPO when using discrete actions\n",
        "              latent_pi, _, _ = model._get_latent(data)\n",
        "              logits = model.action_net(latent_pi)\n",
        "              action_prediction = logits\n",
        "              target = target.long()\n",
        "\n",
        "            loss = criterion(action_prediction, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(\n",
        "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                        epoch,\n",
        "                        batch_idx * len(data),\n",
        "                        len(train_loader.dataset),\n",
        "                        100.0 * batch_idx / len(train_loader),\n",
        "                        loss.item(),\n",
        "                    )\n",
        "                )\n",
        "    def test(model, device, test_loader):\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with th.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                if isinstance(env.action_space, gym.spaces.Box):\n",
        "                  # A2C/PPO policy outputs actions, values, log_prob\n",
        "                  # SAC/TD3 policy outputs actions only\n",
        "                  if isinstance(student, (A2C, PPO)):\n",
        "                    action, _, _ = model(data)\n",
        "                  else:\n",
        "                    # SAC/TD3:\n",
        "                    action = model(data)\n",
        "                  action_prediction = action.double()\n",
        "                else:\n",
        "                  # Retrieve the logits for A2C/PPO when using discrete actions\n",
        "                  latent_pi, _, _ = model._get_latent(data)\n",
        "                  logits = model.action_net(latent_pi)\n",
        "                  action_prediction = logits\n",
        "                  target = target.long()\n",
        "\n",
        "                test_loss = criterion(action_prediction, target)\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
        "    # and testing\n",
        "    train_loader = th.utils.data.DataLoader(\n",
        "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
        "    )\n",
        "    test_loader = th.utils.data.DataLoader(\n",
        "        dataset=test_expert_dataset, batch_size=test_batch_size, shuffle=True, **kwargs,\n",
        "    )\n",
        "\n",
        "    # Define an Optimizer and a learning rate schedule.\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
        "\n",
        "    # Now we are finally ready to train the policy model.\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, optimizer)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    # Implant the trained policy network back into the RL student agent\n",
        "    a2c_student.policy = model"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkEP6i0hEu_R"
      },
      "source": [
        "Evaluate the agent before pretraining, it should be random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7kvYIneEui8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922c3f31-b67e-47ae-f88e-7f9372282313"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward = 33.3 +/- 9.445104552094698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgduZAbF8ao9"
      },
      "source": [
        "\n",
        "\n",
        "Having defined the training procedure we can now run the pretraining!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI1EFFnW8ao-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428d91d9-5111-44dd-a69d-abcaba6f0abd"
      },
      "source": [
        "pretrain_agent(\n",
        "    a2c_student,\n",
        "    epochs=3,\n",
        "    scheduler_gamma=0.7,\n",
        "    learning_rate=1.0,\n",
        "    log_interval=100,\n",
        "    no_cuda=True,\n",
        "    seed=1,\n",
        "    batch_size=64,\n",
        "    test_batch_size=1000,\n",
        ")\n",
        "a2c_student.save(\"a2c_student\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/32000 (0%)]\tLoss: 0.693018\n",
            "Train Epoch: 1 [6400/32000 (20%)]\tLoss: 0.205982\n",
            "Train Epoch: 1 [12800/32000 (40%)]\tLoss: 0.094016\n",
            "Train Epoch: 1 [19200/32000 (60%)]\tLoss: 0.145712\n",
            "Train Epoch: 1 [25600/32000 (80%)]\tLoss: 0.066126\n",
            "Test set: Average loss: 0.0000\n",
            "Train Epoch: 2 [0/32000 (0%)]\tLoss: 0.078968\n",
            "Train Epoch: 2 [6400/32000 (20%)]\tLoss: 0.058335\n",
            "Train Epoch: 2 [12800/32000 (40%)]\tLoss: 0.041778\n",
            "Train Epoch: 2 [19200/32000 (60%)]\tLoss: 0.062527\n",
            "Train Epoch: 2 [25600/32000 (80%)]\tLoss: 0.021562\n",
            "Test set: Average loss: 0.0000\n",
            "Train Epoch: 3 [0/32000 (0%)]\tLoss: 0.033521\n",
            "Train Epoch: 3 [6400/32000 (20%)]\tLoss: 0.027166\n",
            "Train Epoch: 3 [12800/32000 (40%)]\tLoss: 0.033585\n",
            "Train Epoch: 3 [19200/32000 (60%)]\tLoss: 0.069544\n",
            "Train Epoch: 3 [25600/32000 (80%)]\tLoss: 0.012910\n",
            "Test set: Average loss: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3Q5Jm58apE"
      },
      "source": [
        "\n",
        "\n",
        "Finally, let us test how well our RL agent student learned to mimic the behavior of the expert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKZ8O--m8apF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae54b15-72a0-4386-920e-b572760c662f"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean reward = 250.7 +/- 89.63933288462158\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}